{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99641d43",
   "metadata": {},
   "source": [
    "## Clinical Text De-identification \n",
    "\n",
    "This notebook implements three approaches for de-identifying Protected Health Information (PHI) in discharge reports:\n",
    "\n",
    "- Method 1: Basic NER (general domain) + Regex rules\n",
    "- Method 2: Medical domain NER (PHI de-identification) + Regex\n",
    "- Method 3: 3B LLM (no fine-tuning) prompted for de-identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!pip install -q spacy==3.7.5 pandas==2.2.2 numpy==1.26.4 regex==2024.7.24 transformers==4.42.4 datasets==2.20.0 accelerate==0.33.0 torch --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "!python3 -m spacy download en_core_web_lg -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e11e8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 200\n",
      "['note_id', 'subject_id', 'hadm_id', 'note_type', 'note_seq', 'charttime', 'storetime', 'report']\n",
      "Sample chars: 36691\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "DATA_PATH = \"/Users/aryamanbahl/Desktop/IIITH/M25/NLP-H/Assignments/Discharge Reports Dataset.csv\"\n",
    "OUTPUT_DIR = \"/Users/aryamanbahl/Desktop/IIITH/M25/NLP-H/Assignments/part2/\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "notes_df = pd.read_csv(DATA_PATH)\n",
    "print(\"Loaded rows:\", len(notes_df))\n",
    "print(notes_df.columns.tolist())\n",
    "\n",
    "# Keep a subset for demo if dataset is huge\n",
    "MAX_DOCS = 5\n",
    "sample_df = notes_df.head(MAX_DOCS).copy()\n",
    "\n",
    "def get_texts(df: pd.DataFrame) -> List[str]:\n",
    "    return df[\"report\"].astype(str).tolist()\n",
    "\n",
    "raw_texts = get_texts(sample_df)\n",
    "print(\"Sample chars:\", sum(len(t) for t in raw_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4657e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: masking utilities\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Span:\n",
    "    start: int\n",
    "    end: int\n",
    "    label: str\n",
    "\n",
    "PLACEHOLDER_MAP = {\n",
    "    \"PERSON\": \"[NAME]\",\n",
    "    \"ORG\": \"[ORG]\",\n",
    "    \"DATE\": \"[DATE]\",\n",
    "    \"EMAIL\": \"[EMAIL]\",\n",
    "    \"PHONE\": \"[PHONE]\",\n",
    "    \"ID\": \"[ID]\",\n",
    "    \"HOSPITAL\": \"[HOSPITAL]\",\n",
    "    \"LOCATION\": \"[LOCATION]\",\n",
    "}\n",
    "\n",
    "\n",
    "def apply_spans_mask(text: str, spans: List[Span]) -> str:\n",
    "    # Resolve overlaps by sorting and merging\n",
    "    spans_sorted = sorted(spans, key=lambda s: (s.start, -(s.end - s.start)))\n",
    "    merged: List[Span] = []\n",
    "    prev_end = -1\n",
    "    for s in spans_sorted:\n",
    "        if s.start >= prev_end:\n",
    "            merged.append(s)\n",
    "            prev_end = s.end\n",
    "    # Build masked text\n",
    "    out = []\n",
    "    last = 0\n",
    "    for s in merged:\n",
    "        out.append(text[last:s.start])\n",
    "        placeholder = PLACEHOLDER_MAP.get(s.label, f\"[{s.label}]\")\n",
    "        out.append(placeholder)\n",
    "        last = s.end\n",
    "    out.append(text[last:])\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "def save_outputs(tag: str, texts: List[str]):\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"deid_{tag}.jsonl\")\n",
    "    with open(out_path, \"w\") as f:\n",
    "        for t in texts:\n",
    "            f.write(json.dumps({\"text\": t}, ensure_ascii=False) + \"\\n\")\n",
    "    print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd65446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/aryamanbahl/Desktop/IIITH/M25/NLP-H/Assignments/part2/deid_method1_spacy_regex.jsonl\n",
      "Name: [NAME] No: 9295\n",
      "\n",
      "Admission Date: [DATE] Discharge Date: [DATE]\n",
      "\n",
      "Date of Birth: [DATE] Sex: F\n",
      "\n",
      "Service: MEDICINE\n",
      "\n",
      "Allergies:\n",
      "[ORG],Other / Reglan\n",
      "\n",
      "Attending: [NAME].\n",
      "\n",
      "Chief Complaint:\n",
      "Abdominal pain\n",
      "\n",
      "Major [NAME] or Invasive Procedure:\n",
      "None\n",
      "\n",
      "History of Present Illness:\n",
      "Patient is a 1984 yo woman with history of chronic pancreatitis\n",
      "s/p cholecystectomy and sphincterotomy who presents with 1wk of\n",
      "worsening abdominal pain. As per patient, the pain is\n",
      "intermittent, sharp and 0/10 (no pain) in q\n"
     ]
    }
   ],
   "source": [
    "# Method 1: spaCy NER + Regex rules\n",
    "import spacy\n",
    "import regex as re2\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Regex patterns for structured identifiers\n",
    "DATE_PATTERNS = [\n",
    "    r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\",                 # 2025-08-06\n",
    "    r\"\\b\\d{2}/\\d{2}/\\d{4}\\b\",                 # 08/06/2025\n",
    "    r\"\\b\\d{2}-\\d{2}-\\d{4}\\b\",                 # 08-06-2025\n",
    "    r\"\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2},\\s*\\d{4}\\b\",\n",
    "    r\"\\b\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{4}\\b\",\n",
    "]\n",
    "EMAIL_PATTERN = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
    "PHONE_PATTERN = r\"\\b(?:\\+?\\d{1,3}[\\s.-]?)?(?:\\(?\\d{3}\\)?[\\s.-]?)?\\d{3}[\\s.-]?\\d{4}\\b\"\n",
    "ID_PATTERN = r\"\\b(?:MRN|ID|Unit No|Account)[#: ]*\\s*\\d+[\\w-]*\\b\"\n",
    "\n",
    "\n",
    "def spacy_ner_regex_deid(text: str) -> str:\n",
    "    doc = nlp(text)\n",
    "    spans: List[Span] = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"PERSON\", \"ORG\", \"GPE\", \"FAC\", \"LOC\"}:\n",
    "            label = \"PERSON\" if ent.label_ == \"PERSON\" else (\"ORG\" if ent.label_ == \"ORG\" else \"LOCATION\")\n",
    "            spans.append(Span(ent.start_char, ent.end_char, label))\n",
    "    # Regex matches\n",
    "    for pat in DATE_PATTERNS:\n",
    "        for m in re2.finditer(pat, text, flags=re2.IGNORECASE):\n",
    "            spans.append(Span(m.start(), m.end(), \"DATE\"))\n",
    "    for m in re2.finditer(EMAIL_PATTERN, text):\n",
    "        spans.append(Span(m.start(), m.end(), \"EMAIL\"))\n",
    "    for m in re2.finditer(PHONE_PATTERN, text):\n",
    "        spans.append(Span(m.start(), m.end(), \"PHONE\"))\n",
    "    for m in re2.finditer(ID_PATTERN, text):\n",
    "        spans.append(Span(m.start(), m.end(), \"ID\"))\n",
    "    return apply_spans_mask(text, spans)\n",
    "\n",
    "method1_outputs = [spacy_ner_regex_deid(t) for t in raw_texts]\n",
    "save_outputs(\"method1_spacy_regex\", method1_outputs)\n",
    "print(method1_outputs[0][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a1c089",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Method 2: Medical domain NER (HuggingFace) + Regex\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# We choose a PHI de-identification model if available, fallback to biomedical NER\n",
    "# Example candidates: \"obi/deid_roberta_i2b2\" or \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "MODEL_CANDIDATES = [\n",
    "    \"obi/deid_roberta_i2b2\",        # if available\n",
    "    \"gliner-biomed/biomed-ner-large\",  # open biomedical NER\n",
    "    \"emilyalsentzer/Bio_ClinicalBERT\"  # general clinical model (may need token-classification adapter)\n",
    "]\n",
    "\n",
    "loaded = False\n",
    "for name in MODEL_CANDIDATES:\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(name)\n",
    "        mdl = AutoModelForTokenClassification.from_pretrained(name)\n",
    "        nlp_med = pipeline(\"token-classification\", model=mdl, tokenizer=tok, aggregation_strategy=\"simple\")\n",
    "        model_name = name\n",
    "        loaded = True\n",
    "        print(\"Loaded:\", name)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", name, e)\n",
    "\n",
    "if not loaded:\n",
    "    raise RuntimeError(\"Could not load any medical NER model.\")\n",
    "\n",
    "MED_TO_PLACEHOLDER = {\n",
    "    # Common PHI tags in de-id datasets\n",
    "    \"PATIENT\": \"PERSON\",\n",
    "    \"DOCTOR\": \"PERSON\",\n",
    "    \"HOSPITAL\": \"HOSPITAL\",\n",
    "    \"ORGANIZATION\": \"ORG\",\n",
    "    \"LOCATION\": \"LOCATION\",\n",
    "    \"CITY\": \"LOCATION\",\n",
    "    \"STATE\": \"LOCATION\",\n",
    "    \"STREET\": \"LOCATION\",\n",
    "    \"ZIP\": \"LOCATION\",\n",
    "    \"DATE\": \"DATE\",\n",
    "    \"PHONE\": \"PHONE\",\n",
    "    \"FAX\": \"PHONE\",\n",
    "    \"EMAIL\": \"EMAIL\",\n",
    "    \"ID\": \"ID\",\n",
    "}\n",
    "\n",
    "\n",
    "def med_ner_regex_deid(text: str) -> str:\n",
    "    spans: List[Span] = []\n",
    "    preds = nlp_med(text)\n",
    "    for p in preds:\n",
    "        label = p[\"entity_group\"].upper()\n",
    "        label = MED_TO_PLACEHOLDER.get(label, label)\n",
    "        if label in PLACEHOLDER_MAP:\n",
    "            spans.append(Span(int(p[\"start\"]), int(p[\"end\"]), label))\n",
    "    # Reuse regex from method 1\n",
    "    for pat in DATE_PATTERNS:\n",
    "        for m in re2.finditer(pat, text, flags=re2.IGNORECASE):\n",
    "            spans.append(Span(m.start(), m.end(), \"DATE\"))\n",
    "    for m in re2.finditer(EMAIL_PATTERN, text):\n",
    "        spans.append(Span(m.start(), m.end(), \"EMAIL\"))\n",
    "    for m in re2.finditer(PHONE_PATTERN, text):\n",
    "        spans.append(Span(m.start(), m.end(), \"PHONE\"))\n",
    "    for m in re2.finditer(ID_PATTERN, text):\n",
    "        spans.append(Span(m.start(), m.end(), \"ID\"))\n",
    "    return apply_spans_mask(text, spans)\n",
    "\n",
    "method2_outputs = [med_ner_regex_deid(t) for t in raw_texts]\n",
    "save_outputs(\"method2_medner_regex\", method2_outputs)\n",
    "print(model_name, method2_outputs[0][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663cb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: 3B LLM (no fine-tuning) for de-identification\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def run_llm_deid(texts, model_id: str = \"microsoft/phi-3-mini-4k-instruct\"):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32, device_map=\"auto\")\n",
    "\n",
    "    system = (\n",
    "        \"You are a de-identification assistant. Replace all personal identifiers (names, organizations, \"\n",
    "        \"hospitals, locations, phone numbers, dates, emails, account/ID numbers) with appropriate placeholders \"\n",
    "        \"like [NAME], [ORG], [HOSPITAL], [LOCATION], [PHONE], [DATE], [EMAIL], [ID]. Keep clinical content.\")\n",
    "\n",
    "    outputs = []\n",
    "    for txt in texts:\n",
    "        prompt = f\"<|system|>\\n{system}\\n<|user|>\\nText:\\n{txt}\\n\\nReturn only the de-identified text.\\n<|assistant|>\"\n",
    "        inputs = tok(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        gen_ids = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "        out = tok.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        # Extract after assistant tag if present\n",
    "        if \"<|assistant|>\" in out:\n",
    "            out = out.split(\"<|assistant|>\")[-1].strip()\n",
    "        outputs.append(out)\n",
    "    return outputs\n",
    "\n",
    "try:\n",
    "    method3_outputs = run_llm_deid(raw_texts)\n",
    "    save_outputs(\"method3_llm\", method3_outputs)\n",
    "    print(method3_outputs[0][:500])\n",
    "except Exception as e:\n",
    "    print(\"LLM method failed:\", e)\n",
    "    method3_outputs = [\"\"] * len(raw_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e842e6ca",
   "metadata": {},
   "source": [
    "### Pipeline Descriptions\n",
    "\n",
    "- Method 1: spaCy NER to detect PERSON/ORG/LOC + Regex for DATE/EMAIL/PHONE/ID → mask with placeholders.\n",
    "- Method 2: Clinical/PHI NER via transformers token-classification + same Regex rules → mask with placeholders. Falls back among several model IDs.\n",
    "- Method 3: 3B LLM prompted to replace PHI with placeholders; no training or fine-tuning.\n",
    "\n",
    "All outputs saved as `deid_method1_spacy_regex.jsonl`, `deid_method2_medner_regex.jsonl`, and `deid_method3_llm.jsonl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd555dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick comparison and preview\n",
    "from pprint import pprint\n",
    "\n",
    "preview = {\n",
    "    \"method1\": method1_outputs[0][:600] if method1_outputs else \"\",\n",
    "    \"method2\": method2_outputs[0][:600] if method2_outputs else \"\",\n",
    "    \"method3\": method3_outputs[0][:600] if method3_outputs else \"\",\n",
    "}\n",
    "pprint(preview)\n",
    "\n",
    "# Optional: write a side-by-side CSV for the first few docs\n",
    "rows = []\n",
    "for i in range(len(raw_texts)):\n",
    "    rows.append({\n",
    "        \"note_id\": sample_df.iloc[i][\"note_id\"],\n",
    "        \"original\": raw_texts[i],\n",
    "        \"method1\": method1_outputs[i],\n",
    "        \"method2\": method2_outputs[i],\n",
    "        \"method3\": method3_outputs[i],\n",
    "    })\n",
    "pd.DataFrame(rows).to_csv(os.path.join(OUTPUT_DIR, \"deid_comparison_preview.csv\"), index=False)\n",
    "print(\"Saved:\", os.path.join(OUTPUT_DIR, \"deid_comparison_preview.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c25959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple counts of placeholders per method\n",
    "from collections import Counter\n",
    "\n",
    "PH_PLACEHOLDERS = set(PLACEHOLDER_MAP.values())\n",
    "\n",
    "def count_placeholders(text: str) -> Dict[str, int]:\n",
    "    c = Counter()\n",
    "    for ph in PH_PLACEHOLDERS:\n",
    "        c[ph] = text.count(ph)\n",
    "    return dict(c)\n",
    "\n",
    "summary_rows = []\n",
    "for i in range(len(raw_texts)):\n",
    "    row = {\"note_id\": sample_df.iloc[i][\"note_id\"]}\n",
    "    row.update({f\"m1_{k}\": v for k, v in count_placeholders(method1_outputs[i]).items()})\n",
    "    row.update({f\"m2_{k}\": v for k, v in count_placeholders(method2_outputs[i]).items()})\n",
    "    row.update({f\"m3_{k}\": v for k, v in count_placeholders(method3_outputs[i]).items()})\n",
    "    summary_rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"deid_placeholder_counts.csv\")\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(\"Saved:\", summary_path)\n",
    "summary_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e64b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
